{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExtraCredit_SparkML_IRISDataSet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nERsLpiLEiHL",
        "outputId": "f4a307d2-b707-43b6-9509-807160b22e38"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 65kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 39.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=0fb85e9e42afab5b4ae068a4d60799218bd7738f20d02e9db1c98ab6096e6ec2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tfuKNLMEFXP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyspark\n",
        "import os\n",
        "import urllib\n",
        "import sys\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.classification import *\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.ml.feature import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxoH8Gn4Eg9i",
        "outputId": "7bdc5ea3-9785-4439-85ea-3c74814c3f04"
      },
      "source": [
        "spark = pyspark.sql.SparkSession.builder.appName('Iris').getOrCreate()\n",
        "\n",
        "print ('Python version: {}'.format(sys.version))\n",
        "print ('Spark version: {}'.format(spark.version))\n",
        "\n",
        "data = spark.createDataFrame(pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv', header=None, names=['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']))\n",
        "print(\"First 10 rows of Iris dataset:\")\n",
        "data.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version: 3.6.9 (default, Oct  8 2020, 12:12:24) \n",
            "[GCC 8.4.0]\n",
            "Spark version: 3.0.1\n",
            "First 10 rows of Iris dataset:\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "|sepal-length|sepal-width|petal-length|petal-width|      class|\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
            "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|\n",
            "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|\n",
            "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|\n",
            "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|\n",
            "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w7VEO0nE0dc",
        "outputId": "fb8dc6b2-10b6-43c6-ae5c-24e5044622b3"
      },
      "source": [
        "data.printSchema"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.printSchema of DataFrame[sepal-length: double, sepal-width: double, petal-length: double, petal-width: double, class: string]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcMaw5FLGIbl",
        "outputId": "4e26655e-c2e5-4cf1-9848-51f09dcfa7e7"
      },
      "source": [
        "data.show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.show of DataFrame[sepal-length: double, sepal-width: double, petal-length: double, petal-width: double, class: string]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF0ba6HKGLlr",
        "outputId": "3b8f48f2-ed3e-4765-e737-912e106fa777"
      },
      "source": [
        "data.describe().show(5,15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+---------------+---------------+---------------+---------------+--------------+\n",
            "|summary|   sepal-length|    sepal-width|   petal-length|    petal-width|         class|\n",
            "+-------+---------------+---------------+---------------+---------------+--------------+\n",
            "|  count|            150|            150|            150|            150|           150|\n",
            "|   mean|5.8433333333...|3.0540000000...|3.7586666666...|1.1986666666...|          null|\n",
            "| stddev|0.8280661279...|0.4335943113...|1.7644204199...|0.7631607417...|          null|\n",
            "|    min|            4.3|            2.0|            1.0|            0.1|   Iris-setosa|\n",
            "|    max|            7.9|            4.4|            6.9|            2.5|Iris-virginica|\n",
            "+-------+---------------+---------------+---------------+---------------+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y4x006TG0f8"
      },
      "source": [
        "# vectorize all numerical columns into a single feature column\n",
        "feature_cols = data.columns[:-1]\n",
        "assembler = pyspark.ml.feature.VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "data = assembler.transform(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLwSwx8HG2su"
      },
      "source": [
        "# convert text labels into indices\n",
        "data = data.select(['features', 'class'])\n",
        "label_indexer = pyspark.ml.feature.StringIndexer(inputCol='class', outputCol='label').fit(data)\n",
        "data = label_indexer.transform(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehnMOBSAG46Z",
        "outputId": "89a799fb-e2eb-44cf-e48e-84d76ff89c4c"
      },
      "source": [
        "# only select the features and label column\n",
        "data = data.select(['features', 'label'])\n",
        "print(\"Reading for machine learning\")\n",
        "data.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading for machine learning\n",
            "+-----------------+-----+\n",
            "|         features|label|\n",
            "+-----------------+-----+\n",
            "|[5.1,3.5,1.4,0.2]|  0.0|\n",
            "|[4.9,3.0,1.4,0.2]|  0.0|\n",
            "|[4.7,3.2,1.3,0.2]|  0.0|\n",
            "|[4.6,3.1,1.5,0.2]|  0.0|\n",
            "|[5.0,3.6,1.4,0.2]|  0.0|\n",
            "|[5.4,3.9,1.7,0.4]|  0.0|\n",
            "|[4.6,3.4,1.4,0.3]|  0.0|\n",
            "|[5.0,3.4,1.5,0.2]|  0.0|\n",
            "|[4.4,2.9,1.4,0.2]|  0.0|\n",
            "|[4.9,3.1,1.5,0.1]|  0.0|\n",
            "+-----------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkhyRhzCHDDw"
      },
      "source": [
        "# use Logistic Regression to train on the training set\n",
        "train, test = data.randomSplit([0.70, 0.30])\n",
        "lr = pyspark.ml.classification.LogisticRegression(regParam=0.01)\n",
        "model = lr.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI5FCxsUHE5b",
        "outputId": "59109384-6813-46a7-b12b-69f1459d84be"
      },
      "source": [
        "prediction = model.transform(test)\n",
        "print(\"Prediction\")\n",
        "prediction.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction\n",
            "+-----------------+-----+--------------------+--------------------+----------+\n",
            "|         features|label|       rawPrediction|         probability|prediction|\n",
            "+-----------------+-----+--------------------+--------------------+----------+\n",
            "|[4.6,3.1,1.5,0.2]|  0.0|[5.76853978965701...|[0.96722815173249...|       0.0|\n",
            "|[4.6,3.4,1.4,0.3]|  0.0|[6.32679184959567...|[0.98554190859180...|       0.0|\n",
            "|[4.7,3.2,1.6,0.2]|  0.0|[5.77016271259579...|[0.96768338746233...|       0.0|\n",
            "|[4.8,3.0,1.4,0.1]|  0.0|[5.6037764048288,...|[0.94966572389106...|       0.0|\n",
            "|[4.8,3.0,1.4,0.3]|  0.0|[5.17322867975136...|[0.93756072310826...|       0.0|\n",
            "|[4.8,3.1,1.6,0.2]|  0.0|[5.41870286606336...|[0.94930583022936...|       0.0|\n",
            "|[4.8,3.4,1.6,0.2]|  0.0|[6.09466808123252...|[0.97792488230617...|       0.0|\n",
            "|[4.9,2.4,3.3,1.0]|  1.0|[0.33458966464309...|[0.15190041328107...|       1.0|\n",
            "|[5.0,3.2,1.2,0.2]|  0.0|[5.78199121740053...|[0.96131648151059...|       0.0|\n",
            "|[5.0,3.5,1.3,0.3]|  0.0|[6.14512186272275...|[0.97955993247118...|       0.0|\n",
            "+-----------------+-----+--------------------+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GaUXybIHJJh"
      },
      "source": [
        "evaluator = pyspark.ml.evaluation.MulticlassClassificationEvaluator(metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUHhJ3jPHMBg",
        "outputId": "033e0949-fd39-4b46-ad16-b4defc11ef1d"
      },
      "source": [
        "print(\"Accuracy is {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is 0.9583333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}