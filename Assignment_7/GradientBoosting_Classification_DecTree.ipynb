{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientBoosting-Classification-DecTree.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGW6qut59KQc"
      },
      "source": [
        "## Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYRICy4t5cnY"
      },
      "source": [
        "from __future__ import division, print_function\n",
        "from itertools import combinations_with_replacement\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import progressbar\n",
        "import math\n",
        "import sys\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cmx\n",
        "import matplotlib.colors as colors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfgAPVlX9UEK"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrJzCZpY3OPn"
      },
      "source": [
        "def calculate_entropy(y):\n",
        "    \"\"\" Calculate the entropy of label array y \"\"\"\n",
        "    log2 = lambda x: math.log(x) / math.log(2)\n",
        "    unique_labels = np.unique(y)\n",
        "    entropy = 0\n",
        "    for label in unique_labels:\n",
        "        count = len(y[y == label])\n",
        "        p = count / len(y)\n",
        "        entropy += -p * log2(p)\n",
        "    return entropy\n",
        "\n",
        "def calculate_variance(X):\n",
        "    \"\"\" Return the variance of the features in dataset X \"\"\"\n",
        "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
        "    n_samples = np.shape(X)[0]\n",
        "    variance = (1 / n_samples) * np.diag((X - mean).T.dot(X - mean))\n",
        "    \n",
        "    return variance\n",
        "\n",
        "\n",
        "def calculate_std_dev(X):\n",
        "    \"\"\" Calculate the standard deviations of the features in dataset X \"\"\"\n",
        "    std_dev = np.sqrt(calculate_variance(X))\n",
        "    return std_dev\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
        "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def calculate_covariance_matrix(X, Y=None):\n",
        "    \"\"\" Calculate the covariance matrix for the dataset X \"\"\"\n",
        "    if Y is None:\n",
        "        Y = X\n",
        "    n_samples = np.shape(X)[0]\n",
        "    covariance_matrix = (1 / (n_samples-1)) * (X - X.mean(axis=0)).T.dot(Y - Y.mean(axis=0))\n",
        "\n",
        "    return np.array(covariance_matrix, dtype=float)\n",
        " \n",
        "\n",
        "def calculate_correlation_matrix(X, Y=None):\n",
        "    \"\"\" Calculate the correlation matrix for the dataset X \"\"\"\n",
        "    if Y is None:\n",
        "        Y = X\n",
        "    n_samples = np.shape(X)[0]\n",
        "    covariance = (1 / n_samples) * (X - X.mean(0)).T.dot(Y - Y.mean(0))\n",
        "    std_dev_X = np.expand_dims(calculate_std_dev(X), 1)\n",
        "    std_dev_y = np.expand_dims(calculate_std_dev(Y), 1)\n",
        "    correlation_matrix = np.divide(covariance, std_dev_X.dot(std_dev_y.T))\n",
        "\n",
        "    return np.array(correlation_matrix, dtype=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1bddB4Z9BwE"
      },
      "source": [
        "## Plot Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEy346mI4qhF"
      },
      "source": [
        "bar_widgets = [\n",
        "    'Training: ', progressbar.Percentage(), ' ', progressbar.Bar(marker=\"-\", left=\"[\", right=\"]\"),\n",
        "    ' ', progressbar.ETA()\n",
        "]\n",
        "\n",
        "class Plot():\n",
        "    def __init__(self): \n",
        "        self.cmap = plt.get_cmap('viridis')\n",
        "\n",
        "    def _transform(self, X, dim):\n",
        "        covariance = calculate_covariance_matrix(X)\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n",
        "        # Sort eigenvalues and eigenvector by largest eigenvalues\n",
        "        idx = eigenvalues.argsort()[::-1]\n",
        "        eigenvalues = eigenvalues[idx][:dim]\n",
        "        eigenvectors = np.atleast_1d(eigenvectors[:, idx])[:, :dim]\n",
        "        # Project the data onto principal components\n",
        "        X_transformed = X.dot(eigenvectors)\n",
        "\n",
        "        return X_transformed\n",
        "\n",
        "    # Plot the dataset X and the corresponding labels y in 2D using PCA.\n",
        "    def plot_in_2d(self, X, y=None, title=None, accuracy=None, legend_labels=None):\n",
        "        X_transformed = self._transform(X, dim=2)\n",
        "        x1 = X_transformed[:, 0]\n",
        "        x2 = X_transformed[:, 1]\n",
        "        class_distr = []\n",
        "\n",
        "        y = np.array(y).astype(int)\n",
        "\n",
        "        colors = [self.cmap(i) for i in np.linspace(0, 1, len(np.unique(y)))]\n",
        "\n",
        "        # Plot the different class distributions\n",
        "        for i, l in enumerate(np.unique(y)):\n",
        "            _x1 = x1[y == l]\n",
        "            _x2 = x2[y == l]\n",
        "            _y = y[y == l]\n",
        "            class_distr.append(plt.scatter(_x1, _x2, color=colors[i]))\n",
        "\n",
        "        # Plot legend\n",
        "        if not legend_labels is None: \n",
        "            plt.legend(class_distr, legend_labels, loc=1)\n",
        "\n",
        "        # Plot title\n",
        "        if title:\n",
        "            if accuracy:\n",
        "                perc = 100 * accuracy\n",
        "                plt.suptitle(title)\n",
        "                plt.title(\"Accuracy: %.1f%%\" % perc, fontsize=10)\n",
        "            else:\n",
        "                plt.title(title)\n",
        "\n",
        "        # Axis labels\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSDUNim88rk"
      },
      "source": [
        "## Data Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFZDxf3G2oD_"
      },
      "source": [
        "def shuffle_data(X, y, seed=None):\n",
        "    \"\"\" Random shuffle of the samples in X and y \"\"\"\n",
        "    if seed:\n",
        "        np.random.seed(seed)\n",
        "    idx = np.arange(X.shape[0])\n",
        "    np.random.shuffle(idx)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "def divide_on_feature(X, feature_i, threshold):\n",
        "    \"\"\" Divide dataset based on if sample value on feature index is larger than\n",
        "        the given threshold \"\"\"\n",
        "    split_func = None\n",
        "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
        "        split_func = lambda sample: sample[feature_i] >= threshold\n",
        "    else:\n",
        "        split_func = lambda sample: sample[feature_i] == threshold\n",
        "\n",
        "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
        "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
        "\n",
        "    return np.array([X_1, X_2])\n",
        "\n",
        "def standardize(X):\n",
        "    \"\"\" Standardize the dataset X \"\"\"\n",
        "    X_std = X\n",
        "    mean = X.mean(axis=0)\n",
        "    std = X.std(axis=0)\n",
        "    for col in range(np.shape(X)[1]):\n",
        "        if std[col]:\n",
        "            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]\n",
        "    # X_std = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "    return X_std\n",
        "\n",
        "\n",
        "def train_test_split(X, y, test_size=0.5, shuffle=True, seed=None):\n",
        "    \"\"\" Split the data into train and test sets \"\"\"\n",
        "    if shuffle:\n",
        "        X, y = shuffle_data(X, y, seed)\n",
        "    # Split the training data from test data in the ratio specified in\n",
        "    # test_size\n",
        "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
        "    X_train, X_test = X[:split_i], X[split_i:]\n",
        "    y_train, y_test = y[:split_i], y[split_i:]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def to_categorical(x, n_col=None):\n",
        "    \"\"\" One-hot encoding of nominal values \"\"\"\n",
        "    if not n_col:\n",
        "        n_col = np.amax(x) + 1\n",
        "    one_hot = np.zeros((x.shape[0], n_col))\n",
        "    one_hot[np.arange(x.shape[0]), x] = 1\n",
        "    return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQgwxhG-86Nm"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIXKb44M2TCk"
      },
      "source": [
        "class Loss(object):\n",
        "    def loss(self, y_true, y_pred):\n",
        "        return NotImplementedError()\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def acc(self, y, y_pred):\n",
        "        return 0\n",
        "\n",
        "class SquareLoss(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        return 0.5 * np.power((y - y_pred), 2)\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        return -(y - y_pred)\n",
        "\n",
        "class CrossEntropy(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
        "\n",
        "    def acc(self, y, p):\n",
        "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
        "\n",
        "    def gradient(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - (y / p) + (1 - y) / (1 - p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDZLLRUq80U9"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I82NuuK35lH"
      },
      "source": [
        "class DecisionNode():\n",
        "    def __init__(self, feature_i=None, threshold=None,\n",
        "                 value=None, true_branch=None, false_branch=None):\n",
        "        self.feature_i = feature_i          # Index for the feature that is tested\n",
        "        self.threshold = threshold          # Threshold value for feature\n",
        "        self.value = value                  # Value if the node is a leaf in the tree\n",
        "        self.true_branch = true_branch      # 'Left' subtree\n",
        "        self.false_branch = false_branch    # 'Right' subtree\n",
        "\n",
        "\n",
        "# Super class of RegressionTree and ClassificationTree\n",
        "class DecisionTree(object):\n",
        "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
        "                 max_depth=float(\"inf\"), loss=None):\n",
        "        self.root = None  # Root node in dec. tree\n",
        "        # Minimum n of samples to justify split\n",
        "        self.min_samples_split = min_samples_split\n",
        "        # The minimum impurity to justify split\n",
        "        self.min_impurity = min_impurity\n",
        "        # The maximum depth to grow the tree to\n",
        "        self.max_depth = max_depth\n",
        "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
        "        self._impurity_calculation = None\n",
        "        # Function to determine prediction of y at leaf\n",
        "        self._leaf_value_calculation = None\n",
        "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
        "        self.one_dim = None\n",
        "        # If Gradient Boost\n",
        "        self.loss = loss\n",
        "\n",
        "    def fit(self, X, y, loss=None):\n",
        "        \"\"\" Build decision tree \"\"\"\n",
        "        self.one_dim = len(np.shape(y)) == 1\n",
        "        self.root = self._build_tree(X, y)\n",
        "        self.loss=None\n",
        "\n",
        "    def _build_tree(self, X, y, current_depth=0):\n",
        "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
        "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
        "\n",
        "        largest_impurity = 0\n",
        "        best_criteria = None    # Feature index and threshold\n",
        "        best_sets = None        # Subsets of the data\n",
        "\n",
        "        # Check if expansion of y is needed\n",
        "        if len(np.shape(y)) == 1:\n",
        "            y = np.expand_dims(y, axis=1)\n",
        "\n",
        "        # Add y as last column of X\n",
        "        Xy = np.concatenate((X, y), axis=1)\n",
        "\n",
        "        n_samples, n_features = np.shape(X)\n",
        "\n",
        "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
        "            # Calculate the impurity for each feature\n",
        "            for feature_i in range(n_features):\n",
        "                # All values of feature_i\n",
        "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
        "                unique_values = np.unique(feature_values)\n",
        "\n",
        "                # Iterate through all unique values of feature column i and\n",
        "                # calculate the impurity\n",
        "                for threshold in unique_values:\n",
        "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
        "                    # meets the threshold\n",
        "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
        "\n",
        "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
        "                        # Select the y-values of the two sets\n",
        "                        y1 = Xy1[:, n_features:]\n",
        "                        y2 = Xy2[:, n_features:]\n",
        "\n",
        "                        # Calculate impurity\n",
        "                        impurity = self._impurity_calculation(y, y1, y2)\n",
        "\n",
        "                        # If this threshold resulted in a higher information gain than previously\n",
        "                        # recorded save the threshold value and the feature\n",
        "                        # index\n",
        "                        if impurity > largest_impurity:\n",
        "                            largest_impurity = impurity\n",
        "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
        "                            best_sets = {\n",
        "                                \"leftX\": Xy1[:, :n_features],   # X of left subtree\n",
        "                                \"lefty\": Xy1[:, n_features:],   # y of left subtree\n",
        "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
        "                                \"righty\": Xy2[:, n_features:]   # y of right subtree\n",
        "                                }\n",
        "\n",
        "        if largest_impurity > self.min_impurity:\n",
        "            # Build subtrees for the right and left branches\n",
        "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
        "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
        "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
        "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
        "\n",
        "        # We're at leaf => determine value\n",
        "        leaf_value = self._leaf_value_calculation(y)\n",
        "\n",
        "        return DecisionNode(value=leaf_value)\n",
        "\n",
        "\n",
        "    def predict_value(self, x, tree=None):\n",
        "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
        "            value of the leaf that we end up at \"\"\"\n",
        "\n",
        "        if tree is None:\n",
        "            tree = self.root\n",
        "\n",
        "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
        "        if tree.value is not None:\n",
        "            return tree.value\n",
        "\n",
        "        # Choose the feature that we will test\n",
        "        feature_value = x[tree.feature_i]\n",
        "\n",
        "        # Determine if we will follow left or right branch\n",
        "        branch = tree.false_branch\n",
        "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
        "            if feature_value >= tree.threshold:\n",
        "                branch = tree.true_branch\n",
        "        elif feature_value == tree.threshold:\n",
        "            branch = tree.true_branch\n",
        "\n",
        "        # Test subtree\n",
        "        return self.predict_value(x, branch)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
        "        y_pred = [self.predict_value(sample) for sample in X]\n",
        "        return y_pred\n",
        "\n",
        "    def print_tree(self, tree=None, indent=\" \"):\n",
        "        \"\"\" Recursively print the decision tree \"\"\"\n",
        "        if not tree:\n",
        "            tree = self.root\n",
        "\n",
        "        # If we're at leaf => print the label\n",
        "        if tree.value is not None:\n",
        "            print (tree.value)\n",
        "        # Go deeper down the tree\n",
        "        else:\n",
        "            # Print test\n",
        "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
        "            # Print the true scenario\n",
        "            print (\"%sT->\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.true_branch, indent + indent)\n",
        "            # Print the false scenario\n",
        "            print (\"%sF->\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.false_branch, indent + indent)\n",
        "\n",
        "class RegressionTree(DecisionTree):\n",
        "    def _calculate_variance_reduction(self, y, y1, y2):\n",
        "        var_tot = calculate_variance(y)\n",
        "        var_1 = calculate_variance(y1)\n",
        "        var_2 = calculate_variance(y2)\n",
        "        frac_1 = len(y1) / len(y)\n",
        "        frac_2 = len(y2) / len(y)\n",
        "\n",
        "        # Calculate the variance reduction\n",
        "        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
        "\n",
        "        return sum(variance_reduction)\n",
        "\n",
        "    def _mean_of_y(self, y):\n",
        "        value = np.mean(y, axis=0)\n",
        "        return value if len(value) > 1 else value[0]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self._impurity_calculation = self._calculate_variance_reduction\n",
        "        self._leaf_value_calculation = self._mean_of_y\n",
        "        super(RegressionTree, self).fit(X, y)\n",
        "\n",
        "class ClassificationTree(DecisionTree):\n",
        "    def _calculate_information_gain(self, y, y1, y2):\n",
        "        # Calculate information gain\n",
        "        p = len(y1) / len(y)\n",
        "        entropy = calculate_entropy(y)\n",
        "        info_gain = entropy - p * \\\n",
        "            calculate_entropy(y1) - (1 - p) * \\\n",
        "            calculate_entropy(y2)\n",
        "\n",
        "        return info_gain\n",
        "\n",
        "    def _majority_vote(self, y):\n",
        "        most_common = None\n",
        "        max_count = 0\n",
        "        for label in np.unique(y):\n",
        "            # Count number of occurences of samples with label\n",
        "            count = len(y[y == label])\n",
        "            if count > max_count:\n",
        "                most_common = label\n",
        "                max_count = count\n",
        "        return most_common\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self._impurity_calculation = self._calculate_information_gain\n",
        "        self._leaf_value_calculation = self._majority_vote\n",
        "        super(ClassificationTree, self).fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjvxodgy8xEy"
      },
      "source": [
        "## Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xaNS_ym1xaP"
      },
      "source": [
        "class GradientBoosting(object):\n",
        "    def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
        "                 min_impurity, max_depth, regression):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_impurity = min_impurity\n",
        "        self.max_depth = max_depth\n",
        "        self.regression = regression\n",
        "        self.bar = progressbar.ProgressBar(widgets=bar_widgets)\n",
        "        \n",
        "        # Square loss for regression\n",
        "        # Log loss for classification\n",
        "        self.loss = SquareLoss()\n",
        "        if not self.regression:\n",
        "            self.loss = CrossEntropy()\n",
        "\n",
        "        # Initialize regression trees\n",
        "        self.trees = []\n",
        "        for _ in range(n_estimators):\n",
        "            tree = RegressionTree(\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    min_impurity=min_impurity,\n",
        "                    max_depth=self.max_depth)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_pred = np.full(np.shape(y), np.mean(y, axis=0))\n",
        "        for i in self.bar(range(self.n_estimators)):\n",
        "            gradient = self.loss.gradient(y, y_pred)\n",
        "            self.trees[i].fit(X, gradient)\n",
        "            update = self.trees[i].predict(X)\n",
        "            # Update y prediction\n",
        "            y_pred -= np.multiply(self.learning_rate, update)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.array([])\n",
        "        # Make predictions\n",
        "        for tree in self.trees:\n",
        "            update = tree.predict(X)\n",
        "            update = np.multiply(self.learning_rate, update)\n",
        "            y_pred = -update if not y_pred.any() else y_pred - update\n",
        "\n",
        "        if not self.regression:\n",
        "            # Turn into probability distribution\n",
        "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
        "            # Set label to the value that maximizes probability\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "        return y_pred\n",
        "\n",
        "class GradientBoostingClassifier(GradientBoosting):\n",
        "    def __init__(self, n_estimators=200, learning_rate=.5, min_samples_split=2,\n",
        "                 min_info_gain=1e-7, max_depth=2, debug=False):\n",
        "        super(GradientBoostingClassifier, self).__init__(n_estimators=n_estimators, \n",
        "            learning_rate=learning_rate, \n",
        "            min_samples_split=min_samples_split, \n",
        "            min_impurity=min_info_gain,\n",
        "            max_depth=max_depth,\n",
        "            regression=False)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = to_categorical(y)\n",
        "        super(GradientBoostingClassifier, self).fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdD5S5S-8qpc"
      },
      "source": [
        "## Main Function and Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "-lufJY4y1gX_",
        "outputId": "ca686f58-976d-483e-9cec-935cb178b057"
      },
      "source": [
        "def main():\n",
        "\n",
        "    print (\"-- Gradient Boosting Classification --\")\n",
        "\n",
        "    data = datasets.load_iris()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
        "\n",
        "    clf = GradientBoostingClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print (\"Accuracy:\", accuracy)\n",
        "\n",
        "    Plot().plot_in_2d(X_test, y_pred, \n",
        "        title=\"Gradient Boosting\", \n",
        "        accuracy=accuracy, \n",
        "        legend_labels=data.target_names)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training:   1% [                                               ] ETA:   0:00:14"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Gradient Boosting Classification --\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Training: 100% [-----------------------------------------------] Time:  0:00:12\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9833333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}