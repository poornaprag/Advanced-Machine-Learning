{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4e_Reptile_Fomaml.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8-p9uwi1cfj"
      },
      "source": [
        "# FOMAML\n",
        "\n",
        "https://github.com/sudharsan13296/Hands-On-Meta-Learning-With-Python/tree/master/06.%20MAML%20and%20it's%20Variants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bog76Bjr1fOs"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTjIHTnJ8D8o"
      },
      "source": [
        "def sample_points(k):\n",
        "    x = np.random.rand(k,50)\n",
        "    y = np.random.choice([0, 1], size=k, p=[.5, .5]).reshape([-1,1])\n",
        "    return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB-TyadW8HFQ",
        "outputId": "92affcc7-4913-4235-d416-3806c098cf4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x, y = sample_points(10)\n",
        "print (x[0])\n",
        "print (y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.08421146 0.25614285 0.49322005 0.97444157 0.78626646 0.31431982\n",
            " 0.68730212 0.66413857 0.72326233 0.69619232 0.3222493  0.85007957\n",
            " 0.53099714 0.75748931 0.67122891 0.01999594 0.02692064 0.12107096\n",
            " 0.12473492 0.04199738 0.58396786 0.75075736 0.64096524 0.56080669\n",
            " 0.16869642 0.66631172 0.83444604 0.96194575 0.08577885 0.23814892\n",
            " 0.25505483 0.87599263 0.70514429 0.88900334 0.89755285 0.70936201\n",
            " 0.16658511 0.27559298 0.72981688 0.67293064 0.71705568 0.16031785\n",
            " 0.4433588  0.19254843 0.62904425 0.68629745 0.45906516 0.3642726\n",
            " 0.92968542 0.63270318]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYhivjjq8TLQ"
      },
      "source": [
        "class FOMAML(object):\n",
        "    def __init__(self):\n",
        "        \n",
        "        #initialize number of tasks i.e number of tasks we need in each batch of tasks\n",
        "        self.num_tasks = 10\n",
        "        \n",
        "        #number of samples i.e number of shots  -number of data points (k) we need to have in each task\n",
        "        self.num_samples = 10\n",
        "\n",
        "        #number of epochs i.e training iterations\n",
        "        self.epochs = 10000\n",
        "        \n",
        "        #hyperparameter for the inner loop (inner gradient update)\n",
        "        self.alpha = 0.0001\n",
        "        \n",
        "        #hyperparameter for the outer loop (outer gradient update) i.e meta optimization\n",
        "        self.beta = 0.0001\n",
        "       \n",
        "        #randomly initialize our model parameter theta\n",
        "        self.theta = np.random.normal(size=50).reshape(50, 1)\n",
        "      \n",
        "    #define our sigmoid activation function  \n",
        "    def sigmoid(self,a):\n",
        "        return 1.0 / (1 + np.exp(-a))\n",
        "    \n",
        "    \n",
        "    #now let us get to the interesting part i.e training :P\n",
        "    def train(self):\n",
        "        \n",
        "        #for the number of epochs,\n",
        "        for e in range(self.epochs):        \n",
        "            \n",
        "            self.theta_ = []\n",
        "            \n",
        "            #for task i in batch of tasks\n",
        "            for i in range(self.num_tasks):\n",
        "               \n",
        "                #sample k data points and prepare our train set\n",
        "                XTrain, YTrain = sample_points(self.num_samples)\n",
        "                \n",
        "                a = np.matmul(XTrain, self.theta)\n",
        "\n",
        "                YHat = self.sigmoid(a)\n",
        "\n",
        "                #since we are performing classification, we use cross entropy loss as our loss function\n",
        "                loss = ((np.matmul(-YTrain.T, np.log(YHat)) - np.matmul((1 -YTrain.T), np.log(1 - YHat)))/self.num_samples)[0][0]\n",
        "                \n",
        "                #minimize the loss by calculating gradients\n",
        "                gradient = np.matmul(XTrain.T, (YHat - YTrain)) / self.num_samples\n",
        "\n",
        "                #update the gradients and find the optimal parameter theta' for each of tasks\n",
        "                self.theta_.append(self.theta - self.alpha*gradient)\n",
        "                \n",
        "     \n",
        "            #initialize meta gradients\n",
        "            meta_gradient = np.zeros(self.theta.shape)\n",
        "                        \n",
        "            for i in range(self.num_tasks):\n",
        "            \n",
        "                #sample k data points and prepare our test set for meta training\n",
        "                XTest, YTest = sample_points(10)\n",
        "\n",
        "                #predict the value of y\n",
        "                a = np.matmul(XTest, self.theta_[i])\n",
        "                \n",
        "                YPred = self.sigmoid(a)\n",
        "                           \n",
        "                #compute meta gradients\n",
        "                meta_gradient += np.matmul(XTest.T, (YPred - YTest)) / self.num_samples\n",
        "\n",
        "  \n",
        "            #update our randomly initialized model parameter theta with the meta gradients\n",
        "            self.theta = self.theta-self.beta*meta_gradient/self.num_tasks\n",
        "                                       \n",
        "            if e%1000==0:\n",
        "                print (\"Epoch {}: Loss {}\\n\".format(e,loss) )            \n",
        "                print ('Updated Model Parameter Theta\\n')\n",
        "                print ('Sampling Next Batch of Tasks \\n')\n",
        "                print ('---------------------------------\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9YgiBxw8m9p",
        "outputId": "4a8511ba-385b-499a-dbec-4fc97c41b641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = FOMAML()\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss 2.0908660669408943\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 1000: Loss 1.3666986065612197\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 2000: Loss 0.5886842427263194\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 3000: Loss 1.54862949907957\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 4000: Loss 0.7983253114820272\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 5000: Loss 0.7820201318199596\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 6000: Loss 0.7996105093213443\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 7000: Loss 0.8340660067359403\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 8000: Loss 2.145034077096864\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 9000: Loss 1.7938071956866761\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}